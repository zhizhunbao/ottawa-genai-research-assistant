#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ottawa.ca PDF ä¸‹è½½è„šæœ¬
ç”¨äºè‡ªåŠ¨ä¸‹è½½ Ottawa Economic Development Update PDFs (Q1 2022 - Q4 2025)

ä½¿ç”¨æ–¹æ³•:
    python download_ottawa_pdfs.py [--output-dir OUTPUT_DIR] [--year YEAR] [--quarter QUARTER]
    python download_ottawa_pdfs.py --list-urls  # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ PDF URL
    python download_ottawa_pdfs.py --all  # ä¸‹è½½æ‰€æœ‰ PDF

ç¤ºä¾‹:
    python download_ottawa_pdfs.py --year 2024 --quarter Q1
    python download_ottawa_pdfs.py --all --output-dir ../backend/uploads
"""

import argparse
import json
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# å°è¯•å¯¼å…¥ Seleniumï¼ˆå¯é€‰ï¼‰
try:
    from selenium import webdriver  # type: ignore
    from selenium.webdriver.chrome.options import Options  # type: ignore
    from selenium.webdriver.common.by import By  # type: ignore
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False


def find_project_root() -> Path:
    """
    è‡ªåŠ¨æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŒ…å« backend ç›®å½•çš„ç›®å½•ï¼‰
    
    Returns:
        é¡¹ç›®æ ¹ç›®å½•çš„ Path å¯¹è±¡
    """
    # ä»è„šæœ¬æ–‡ä»¶ä½ç½®å¼€å§‹
    script_dir = Path(__file__).parent.resolve()
    current = script_dir
    
    # å‘ä¸ŠæŸ¥æ‰¾åŒ…å« backend ç›®å½•çš„ç›®å½•
    while current != current.parent:
        backend_dir = current / "backend"
        if backend_dir.exists() and backend_dir.is_dir():
            return current
        current = current.parent
    
    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆå‡è®¾ scripts åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼‰
    return script_dir.parent


class OttawaPDFDownloader:
    """Ottawa.ca PDF ä¸‹è½½å™¨"""

    def __init__(self, output_dir: Optional[str] = None, timeout: int = 30):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨

        Args:
            output_dir: PDF ä¿å­˜ç›®å½•ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™è‡ªåŠ¨æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ backend/uploadsï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        if output_dir is None:
            # è‡ªåŠ¨æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•
            project_root = find_project_root()
            output_dir = project_root / "backend" / "uploads"
        else:
            output_dir = Path(output_dir)
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•ä»é¡¹ç›®æ ¹ç›®å½•è§£æ
            if not output_dir.is_absolute():
                project_root = find_project_root()
                # å¦‚æœè·¯å¾„ä»¥ .. å¼€å¤´ï¼Œè¯´æ˜æ˜¯ä» scripts ç›®å½•çš„ç›¸å¯¹è·¯å¾„
                if str(output_dir).startswith(".."):
                    output_dir = project_root / "backend" / "uploads"
                else:
                    output_dir = project_root / output_dir
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.timeout = timeout

        # é…ç½®é‡è¯•ç­–ç•¥
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "HEAD"],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # è®¾ç½® User-Agent
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        })

        # Economic Development Update PDFs URL æ¨¡å¼
        # å®é™… URL æ ¼å¼: https://documents.ottawa.ca/sites/default/files/economic_update_q{quarter}_{year}_en.pdf
        self.base_url = "https://documents.ottawa.ca/sites/default/files"
        self.source_page_url = "https://ottawa.ca/en/planning-development-and-construction/housing-and-development-reports/local-economic-development-information/economic-development-update"
        # anchor_ids ä¸å†éœ€è¦ç¡¬ç¼–ç ï¼Œä¼šè‡ªåŠ¨å‘ç°æ‰€æœ‰å¯æŠ˜å åŒºåŸŸ
        self.pdf_urls = self._generate_pdf_urls()

    def _extract_pdf_links_from_webpage(self) -> Dict[str, str]:
        """
        ä»ç½‘é¡µæå– PDF é“¾æ¥ï¼ˆä½¿ç”¨ Selenium è‡ªåŠ¨å±•å¼€æ‰€æœ‰å¯æŠ˜å åŒºåŸŸï¼‰
        è‡ªåŠ¨å‘ç°é¡µé¢ä¸Šçš„æ‰€æœ‰å¯æŠ˜å æŒ‰é’®å¹¶å±•å¼€å®ƒä»¬
        
        Returns:
            Dict[year_quarter, pdf_url]: æå–åˆ°çš„ PDF URL æ˜ å°„
        """
        extracted_urls = {}
        
        if not SELENIUM_AVAILABLE:
            print("âš ï¸  Selenium æœªå®‰è£…ï¼Œæ— æ³•ä»ç½‘é¡µæå–é“¾æ¥ï¼Œå°†ä½¿ç”¨ç¡¬ç¼–ç  URL")
            return extracted_urls
        
        print("ğŸ” å°è¯•ä»ç½‘é¡µæå– PDF é“¾æ¥...")
        driver = None
        try:
            # é…ç½® Chrome é€‰é¡¹
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            driver = webdriver.Chrome(options=chrome_options)
            driver.get(self.source_page_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)
            
            # è‡ªåŠ¨å‘ç°æ‰€æœ‰å¯æŠ˜å æŒ‰é’®
            print("\nğŸ” è‡ªåŠ¨å‘ç°æ‰€æœ‰å¯æŠ˜å åŒºåŸŸ...")
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å¯æŠ˜å æŒ‰é’®
            # æ–¹æ³•1: æŸ¥æ‰¾å¸¦æœ‰ data-toggle="collapse" çš„æŒ‰é’®
            collapsible_buttons = driver.find_elements(
                By.CSS_SELECTOR, 
                "button[data-toggle='collapse'], button[data-target^='#'], button[aria-controls]"
            )
            
            # æ–¹æ³•2: æŸ¥æ‰¾ aria-expanded="false" çš„æŒ‰é’®ï¼ˆæœªå±•å¼€çš„ï¼‰
            collapsed_buttons = driver.find_elements(
                By.CSS_SELECTOR,
                "button[aria-expanded='false']"
            )
            
            # åˆå¹¶å¹¶å»é‡ï¼ˆé€šè¿‡æŒ‰é’®çš„å®šä½ä¿¡æ¯ï¼‰
            all_buttons = []
            seen_buttons = set()
            
            for btn in collapsible_buttons + collapsed_buttons:
                try:
                    # ä½¿ç”¨æŒ‰é’®çš„ä½ç½®å’Œå±æ€§ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    btn_id = btn.get_attribute('id') or ''
                    data_target = btn.get_attribute('data-target') or ''
                    aria_controls = btn.get_attribute('aria-controls') or ''
                    btn_text = btn.text[:50] if btn.text else ''  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                    
                    # åˆ›å»ºå”¯ä¸€æ ‡è¯†
                    unique_id = f"{btn_id}|{data_target}|{aria_controls}|{btn_text}"
                    
                    if unique_id not in seen_buttons:
                        seen_buttons.add(unique_id)
                        all_buttons.append(btn)
                except Exception:
                    continue
            
            print(f"âœ“ æ‰¾åˆ° {len(all_buttons)} ä¸ªå¯æŠ˜å æŒ‰é’®")
            
            # å±•å¼€æ‰€æœ‰æ‰¾åˆ°çš„æŒ‰é’®
            expanded_count = 0
            for idx, btn in enumerate(all_buttons, 1):
                try:
                    # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å·²ç»å±•å¼€
                    aria_expanded = btn.get_attribute('aria-expanded')
                    if aria_expanded == 'true':
                        continue  # å·²ç»å±•å¼€ï¼Œè·³è¿‡
                    
                    # è·å–æŒ‰é’®ä¿¡æ¯ç”¨äºæ—¥å¿—
                    data_target = btn.get_attribute('data-target') or ''
                    aria_controls = btn.get_attribute('aria-controls') or ''
                    btn_info = data_target or aria_controls or f"æŒ‰é’® #{idx}"
                    
                    print(f"  [{idx}/{len(all_buttons)}] å±•å¼€: {btn_info}")
                    
                    # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®
                    driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", btn)
                    time.sleep(0.3)
                    
                    # ç‚¹å‡»æŒ‰é’®
                    driver.execute_script("arguments[0].click();", btn)
                    time.sleep(1)  # ç­‰å¾…å†…å®¹å±•å¼€
                    
                    expanded_count += 1
                except Exception as e:
                    print(f"  âš ï¸  å±•å¼€æŒ‰é’® #{idx} æ—¶å‡ºé”™: {e}")
                    continue
            
            print(f"\nâœ“ æˆåŠŸå±•å¼€ {expanded_count} ä¸ªå¯æŠ˜å åŒºåŸŸ")
            
            # ç­‰å¾…æ‰€æœ‰å†…å®¹åŠ è½½
            time.sleep(2)
            
            # æå–æ‰€æœ‰ PDF é“¾æ¥ï¼ˆä»æ•´ä¸ªé¡µé¢ï¼ŒåŒ…æ‹¬æ‰€æœ‰å·²å±•å¼€çš„åŒºåŸŸï¼‰
            page_source = driver.page_source
            pdf_pattern = re.compile(r'https?://[^\s"\'<>]+\.pdf', re.IGNORECASE)
            all_pdf_urls = pdf_pattern.findall(page_source)
            
            print(f"\nâœ“ æ‰¾åˆ° {len(all_pdf_urls)} ä¸ª PDF é“¾æ¥")
            
            # è§£æé“¾æ¥ï¼ŒåŒ¹é…å¹´ä»½å’Œå­£åº¦
            for pdf_url in all_pdf_urls:
                # æå–å¹´ä»½å’Œå­£åº¦ä¿¡æ¯
                year_match = re.search(r'(\d{4})', pdf_url)
                quarter_match = re.search(r'[qQ]([1-4])', pdf_url)
                
                if year_match and quarter_match:
                    year = int(year_match.group(1))
                    quarter_num = int(quarter_match.group(1))
                    quarter = f"Q{quarter_num}"
                    
                    # åªå¤„ç† 2022-2025 å¹´çš„é“¾æ¥
                    if 2022 <= year <= 2025:
                        key = f"{year}_{quarter}"
                        # å¦‚æœåŒä¸€ä¸ª key æœ‰å¤šä¸ª URLï¼Œä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆæˆ–å¯ä»¥è®°å½•è­¦å‘Šï¼‰
                        if key not in extracted_urls:
                            extracted_urls[key] = pdf_url
                            print(f"  âœ“ {key}: {pdf_url}")
                        else:
                            print(f"  âš ï¸  {key} å·²æœ‰ URLï¼Œè·³è¿‡é‡å¤: {pdf_url}")
            
        except Exception as e:
            print(f"âš ï¸  ä»ç½‘é¡µæå–é“¾æ¥å¤±è´¥: {e}")
        finally:
            if driver:
                driver.quit()
        
        return extracted_urls

    def _generate_pdf_urls(self) -> Dict[str, Dict[str, str]]:
        """
        ç”Ÿæˆ PDF URL åˆ—è¡¨
        ä¼˜å…ˆä»ç½‘é¡µæå–ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç¡¬ç¼–ç  URL
        æ ¹æ® PRDï¼Œéœ€è¦ä¸‹è½½ Q1 2022 - Q4 2025 çš„æŠ¥å‘Š

        Returns:
            Dict[year_quarter, Dict]: PDF URL å’Œå…ƒæ•°æ®
        """
        # é¦–å…ˆå°è¯•ä»ç½‘é¡µæå–é“¾æ¥
        extracted_urls = self._extract_pdf_links_from_webpage()
        
        urls = {}

        # Q1 2022 - Q4 2025
        for year in range(2022, 2026):
            for quarter_num, quarter in enumerate(["Q1", "Q2", "Q3", "Q4"], 1):
                # è·³è¿‡ 2025 å¹´ Q4 ä¹‹åï¼ˆå¦‚æœå½“å‰æ—¥æœŸè¿˜æ²¡åˆ°ï¼‰
                if year == 2025 and quarter == "Q4":
                    current_date = datetime.now()
                    if current_date.month < 10:  # Q4 é€šå¸¸åœ¨ 10 æœˆä¹‹åå‘å¸ƒ
                        continue

                key = f"{year}_{quarter}"
                
                # ä¼˜å…ˆä½¿ç”¨ä»ç½‘é¡µæå–çš„ URL
                if key in extracted_urls:
                    pdf_url = extracted_urls[key]
                else:
                    # å›é€€åˆ°ç¡¬ç¼–ç  URL
                    pdf_url = f"{self.base_url}/economic_update_q{quarter_num}_{year}_en.pdf"

                urls[key] = {
                    "year": year,
                    "quarter": quarter,
                    "possible_urls": [pdf_url],
                    "final_url": pdf_url,
                    "filename": f"Economic_Development_Update_{quarter}_{year}.pdf",
                    "title": f"Economic Development Update {quarter} {year}",
                }

        return urls

    def _find_pdf_url(self, possible_urls: List[str]) -> Optional[str]:
        """
        å°è¯•æ‰¾åˆ°æœ‰æ•ˆçš„ PDF URL

        Args:
            possible_urls: å¯èƒ½çš„ URL åˆ—è¡¨

        Returns:
            æœ‰æ•ˆçš„ PDF URL æˆ– None
        """
        for url in possible_urls:
            try:
                # å°è¯•ç›´æ¥è®¿é—® PDF
                full_url = urljoin(self.base_url, url)
                response = self.session.head(full_url, timeout=self.timeout, allow_redirects=True)
                
                if response.status_code == 200:
                    content_type = response.headers.get("Content-Type", "").lower()
                    if "pdf" in content_type:
                        return full_url

                # å¦‚æœä¸æ˜¯ PDFï¼Œå°è¯•åœ¨é¡µé¢ä¸­æŸ¥æ‰¾ PDF é“¾æ¥
                if not url.endswith(".pdf"):
                    response = self.session.get(full_url, timeout=self.timeout)
                    if response.status_code == 200:
                        # æŸ¥æ‰¾é¡µé¢ä¸­çš„ PDF é“¾æ¥
                        pdf_links = re.findall(
                            r'href=["\']([^"\']*\.pdf[^"\']*)["\']',
                            response.text,
                            re.IGNORECASE
                        )
                        if pdf_links:
                            # è¿”å›ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„ PDF é“¾æ¥
                            pdf_url = pdf_links[0]
                            if not pdf_url.startswith("http"):
                                pdf_url = urljoin(full_url, pdf_url)
                            return pdf_url

            except requests.RequestException as e:
                print(f"  âš ï¸  å°è¯• URL {url} å¤±è´¥: {e}")
                continue

        return None

    def _extract_metadata_from_filename(self, filename: str) -> Dict[str, str]:
        """
        ä»æ–‡ä»¶åæå–å…ƒæ•°æ®

        Args:
            filename: PDF æ–‡ä»¶å

        Returns:
            å…ƒæ•°æ®å­—å…¸
        """
        metadata = {
            "source": "ottawa.ca",
            "document_type": "Economic Development Update",
            "upload_date": datetime.now().isoformat(),
        }

        # æå–å¹´ä»½å’Œå­£åº¦
        year_match = re.search(r"(\d{4})", filename)
        quarter_match = re.search(r"(Q[1-4])", filename, re.IGNORECASE)

        if year_match:
            metadata["year"] = year_match.group(1)
        if quarter_match:
            metadata["quarter"] = quarter_match.group(1).upper()

        return metadata

    def download_pdf(
        self, url: str, filename: str, metadata: Optional[Dict] = None
    ) -> Tuple[bool, str]:
        """
        ä¸‹è½½å•ä¸ª PDF æ–‡ä»¶

        Args:
            url: PDF URL
            filename: ä¿å­˜çš„æ–‡ä»¶å
            metadata: å¯é€‰çš„å…ƒæ•°æ®

        Returns:
            (æˆåŠŸæ ‡å¿—, æ–‡ä»¶è·¯å¾„æˆ–é”™è¯¯æ¶ˆæ¯)
        """
        try:
            print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½: {filename}")
            print(f"   URL: {url}")

            response = self.session.get(url, timeout=self.timeout, stream=True)
            response.raise_for_status()

            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get("Content-Type", "").lower()
            if "pdf" not in content_type:
                print(f"  âš ï¸  è­¦å‘Š: å†…å®¹ç±»å‹ä¸æ˜¯ PDF ({content_type})")

            # ä¿å­˜æ–‡ä»¶
            file_path = self.output_dir / filename
            total_size = int(response.headers.get("Content-Length", 0))

            with open(file_path, "wb") as f:
                downloaded = 0
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            percent = (downloaded / total_size) * 100
                            print(f"\r   è¿›åº¦: {percent:.1f}% ({downloaded}/{total_size} bytes)", end="")

            print()  # æ¢è¡Œ
            print(f"  âœ… ä¸‹è½½å®Œæˆ: {file_path}")

            # ä¿å­˜å…ƒæ•°æ®ï¼ˆå¦‚æœæä¾›ï¼‰
            if metadata:
                metadata_file = file_path.with_suffix(".json")
                with open(metadata_file, "w", encoding="utf-8") as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)
                print(f"  ğŸ“„ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")

            return True, str(file_path)

        except requests.RequestException as e:
            error_msg = f"ä¸‹è½½å¤±è´¥: {str(e)}"
            print(f"  âŒ {error_msg}")
            return False, error_msg
        except Exception as e:
            error_msg = f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}"
            print(f"  âŒ {error_msg}")
            return False, error_msg

    def download_by_quarter(self, year: int, quarter: str) -> bool:
        """
        ä¸‹è½½æŒ‡å®šå­£åº¦å’Œå¹´ä»½çš„ PDF

        Args:
            year: å¹´ä»½
            quarter: å­£åº¦ (Q1, Q2, Q3, Q4)

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        key = f"{year}_{quarter}"
        if key not in self.pdf_urls:
            print(f"âŒ æœªæ‰¾åˆ° {quarter} {year} çš„ PDF é…ç½®")
            return False

        pdf_info = self.pdf_urls[key]
        print(f"\nğŸ” ä¸‹è½½ {pdf_info['title']}...")

        # ç›´æ¥ä½¿ç”¨å·²è®¾ç½®çš„ URL
        final_url = pdf_info["final_url"]

        # æå–å…ƒæ•°æ®
        metadata = self._extract_metadata_from_filename(pdf_info["filename"])
        metadata.update({
            "year": str(year),
            "quarter": quarter,
            "title": pdf_info["title"],
            "source_url": final_url,
        })

        # ä¸‹è½½
        success, result = self.download_pdf(
            final_url, pdf_info["filename"], metadata
        )

        return success

    def download_all(self) -> Dict[str, int]:
        """
        ä¸‹è½½æ‰€æœ‰å¯ç”¨çš„ PDF

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {"total": 0, "success": 0, "failed": 0, "not_found": 0}

        print("\nğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½ Ottawa Economic Development Updates")
        print(f"ğŸ“ ä¿å­˜ç›®å½•: {self.output_dir}")
        print(f"ğŸ“Š æ€»è®¡: {len(self.pdf_urls)} ä¸ª PDF\n")

        for key, pdf_info in self.pdf_urls.items():
            stats["total"] += 1
            year = pdf_info["year"]
            quarter = pdf_info["quarter"]

            print(f"\n[{stats['total']}/{len(self.pdf_urls)}] {pdf_info['title']}")

            # ç›´æ¥ä½¿ç”¨å·²è®¾ç½®çš„ URL
            final_url = pdf_info["final_url"]

            # æå–å…ƒæ•°æ®
            metadata = self._extract_metadata_from_filename(pdf_info["filename"])
            metadata.update({
                "year": str(year),
                "quarter": quarter,
                "title": pdf_info["title"],
                "source_url": final_url,
            })

            # ä¸‹è½½
            success, _ = self.download_pdf(final_url, pdf_info["filename"], metadata)
            if success:
                stats["success"] += 1
            else:
                stats["failed"] += 1

            time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«

        return stats

    def list_urls(self) -> None:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ PDF URL"""
        print("\nğŸ“‹ Ottawa Economic Development Update PDFs (Q1 2022 - Q4 2025)\n")
        print(f"{'å¹´ä»½':<8} {'å­£åº¦':<6} {'æ–‡ä»¶å':<50} {'çŠ¶æ€':<10} {'URL':<60}")
        print("-" * 140)

        for key, pdf_info in sorted(self.pdf_urls.items()):
            year = pdf_info["year"]
            quarter = pdf_info["quarter"]
            filename = pdf_info["filename"]
            url = pdf_info["final_url"]
            status = "å¾…ä¸‹è½½"

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            file_path = self.output_dir / filename
            if file_path.exists():
                status = "å·²å­˜åœ¨"

            print(f"{year:<8} {quarter:<6} {filename:<50} {status:<10} {url:<60}")

        print(f"\nğŸ“ ä¿å­˜ç›®å½•: {self.output_dir}")
        print(f"ğŸ“Š æ€»è®¡: {len(self.pdf_urls)} ä¸ª PDF\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸‹è½½ Ottawa.ca Economic Development Update PDFs",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ PDF
  python download_ottawa_pdfs.py --list-urls

  # ä¸‹è½½æ‰€æœ‰ PDF
  python download_ottawa_pdfs.py --all

  # ä¸‹è½½æŒ‡å®šå¹´ä»½å’Œå­£åº¦çš„ PDF
  python download_ottawa_pdfs.py --year 2024 --quarter Q1

  # ä¸‹è½½æŒ‡å®šå¹´ä»½çš„æ‰€æœ‰å­£åº¦
  python download_ottawa_pdfs.py --year 2024

  # æŒ‡å®šè¾“å‡ºç›®å½•
  python download_ottawa_pdfs.py --all --output-dir ../backend/uploads
        """,
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="PDF ä¿å­˜ç›®å½• (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ backend/uploads)",
    )

    parser.add_argument(
        "--year",
        type=int,
        help="ä¸‹è½½æŒ‡å®šå¹´ä»½çš„ PDF (ä¾‹å¦‚: 2024)",
    )

    parser.add_argument(
        "--quarter",
        type=str,
        choices=["Q1", "Q2", "Q3", "Q4"],
        help="ä¸‹è½½æŒ‡å®šå­£åº¦çš„ PDF (éœ€è¦é…åˆ --year ä½¿ç”¨)",
    )

    parser.add_argument(
        "--all",
        action="store_true",
        help="ä¸‹è½½æ‰€æœ‰å¯ç”¨çš„ PDF (Q1 2022 - Q4 2025)",
    )

    parser.add_argument(
        "--list-urls",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ PDF URL å’ŒçŠ¶æ€",
    )

    parser.add_argument(
        "--timeout",
        type=int,
        default=30,
        help="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰(é»˜è®¤: 30)",
    )

    args = parser.parse_args()

    # åˆ›å»ºä¸‹è½½å™¨
    downloader = OttawaPDFDownloader(
        output_dir=args.output_dir, timeout=args.timeout
    )

    try:
        if args.list_urls:
            # åˆ—å‡ºæ‰€æœ‰ URL
            downloader.list_urls()

        elif args.all:
            # ä¸‹è½½æ‰€æœ‰ PDF
            stats = downloader.download_all()
            print("\n" + "=" * 80)
            print("ğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
            print(f"  æ€»è®¡: {stats['total']}")
            print(f"  âœ… æˆåŠŸ: {stats['success']}")
            print(f"  âŒ å¤±è´¥: {stats['failed']}")
            print(f"  âš ï¸  æœªæ‰¾åˆ°: {stats['not_found']}")
            print("=" * 80)

        elif args.year:
            if args.quarter:
                # ä¸‹è½½æŒ‡å®šå­£åº¦
                success = downloader.download_by_quarter(args.year, args.quarter)
                sys.exit(0 if success else 1)
            else:
                # ä¸‹è½½æŒ‡å®šå¹´ä»½çš„æ‰€æœ‰å­£åº¦
                quarters = ["Q1", "Q2", "Q3", "Q4"]
                success_count = 0
                for quarter in quarters:
                    if downloader.download_by_quarter(args.year, quarter):
                        success_count += 1
                    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

                print(f"\nğŸ“Š å®Œæˆ: {success_count}/{len(quarters)} ä¸ªå­£åº¦ä¸‹è½½æˆåŠŸ")
                sys.exit(0 if success_count > 0 else 1)

        else:
            parser.print_help()
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

