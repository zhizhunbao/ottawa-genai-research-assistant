"""
Model Runtime - Unified LLM provider interface.

@module backend/ai/model-runtime
@source dify/api/core/model_runtime/model_provider/ (provider pattern)
@reference https://github.com/langgenius/dify
@template S4-M4-F2

Unified model runtime that abstracts away different LLM providers:
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude)
- Local models (Ollama)
- Custom providers

Pattern: provider registry + model configuration + unified generate interface.
"""

from __future__ import annotations

import asyncio
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, AsyncIterator

logger = logging.getLogger(__name__)


# ─── Types ───────────────────────────────────────────────────────────────────

class ModelType(str, Enum):
    LLM = "llm"
    EMBEDDING = "embedding"
    SPEECH2TEXT = "speech2text"
    TEXT2SPEECH = "text2speech"
    RERANK = "rerank"
    IMAGE = "image"


@dataclass
class ModelConfig:
    """Configuration for a specific model."""
    model_id: str
    provider: str
    model_type: ModelType
    display_name: str = ""
    context_window: int = 4096
    max_output: int = 4096
    supports_streaming: bool = True
    supports_vision: bool = False
    supports_function_calling: bool = False
    pricing: dict[str, float] = field(default_factory=dict)  # per 1K tokens
    extra: dict[str, Any] = field(default_factory=dict)


@dataclass
class GenerateRequest:
    """Request for LLM generation."""
    messages: list[dict[str, Any]]
    model: str = ""
    temperature: float = 0.7
    max_tokens: int | None = None
    top_p: float = 1.0
    stop: list[str] | None = None
    stream: bool = False
    tools: list[dict] | None = None
    extra: dict[str, Any] = field(default_factory=dict)


@dataclass
class GenerateResponse:
    """Response from LLM generation."""
    content: str
    model: str
    usage: dict[str, int] = field(default_factory=dict)
    finish_reason: str = "stop"
    tool_calls: list[dict] | None = None


# ─── Provider Interface ─────────────────────────────────────────────────────
# @source dify/api/core/model_runtime/model_provider/

class ModelProvider(ABC):
    """Abstract base for LLM providers."""
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Provider name (e.g., 'openai', 'anthropic')."""
        ...
    
    @abstractmethod
    def list_models(self) -> list[ModelConfig]:
        """List available models for this provider."""
        ...
    
    @abstractmethod
    async def generate(self, request: GenerateRequest) -> GenerateResponse:
        """Generate a completion."""
        ...
    
    @abstractmethod
    async def generate_stream(
        self, request: GenerateRequest
    ) -> AsyncIterator[str]:
        """Generate a streaming completion."""
        ...
    
    async def embed(self, texts: list[str], model: str = "") -> list[list[float]]:
        """Generate embeddings. Not all providers support this."""
        raise NotImplementedError(f"{self.name} does not support embeddings")


# ─── OpenAI Provider ────────────────────────────────────────────────────────

class OpenAIProvider(ModelProvider):
    """OpenAI/OpenAI-compatible provider."""
    
    def __init__(
        self,
        api_key: str,
        base_url: str = "https://api.openai.com/v1",
        org_id: str | None = None,
    ):
        self.api_key = api_key
        self.base_url = base_url
        self.org_id = org_id
    
    @property
    def name(self) -> str:
        return "openai"
    
    def list_models(self) -> list[ModelConfig]:
        return [
            ModelConfig("gpt-4o", "openai", ModelType.LLM, "GPT-4o",
                       context_window=128000, supports_vision=True, supports_function_calling=True),
            ModelConfig("gpt-4o-mini", "openai", ModelType.LLM, "GPT-4o Mini",
                       context_window=128000, supports_function_calling=True),
            ModelConfig("gpt-3.5-turbo", "openai", ModelType.LLM, "GPT-3.5 Turbo",
                       context_window=16385, supports_function_calling=True),
            ModelConfig("text-embedding-3-small", "openai", ModelType.EMBEDDING,
                       "Embedding 3 Small"),
            ModelConfig("text-embedding-3-large", "openai", ModelType.EMBEDDING,
                       "Embedding 3 Large"),
        ]
    
    async def generate(self, request: GenerateRequest) -> GenerateResponse:
        """Generate using OpenAI API."""
        try:
            from openai import AsyncOpenAI
        except ImportError:
            raise ImportError("pip install openai")
        
        client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)
        
        kwargs: dict[str, Any] = {
            "model": request.model or "gpt-4o-mini",
            "messages": request.messages,
            "temperature": request.temperature,
            "top_p": request.top_p,
        }
        if request.max_tokens:
            kwargs["max_tokens"] = request.max_tokens
        if request.stop:
            kwargs["stop"] = request.stop
        if request.tools:
            kwargs["tools"] = request.tools
        
        response = await client.chat.completions.create(**kwargs)
        choice = response.choices[0]
        
        return GenerateResponse(
            content=choice.message.content or "",
            model=response.model,
            usage={
                "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                "total_tokens": response.usage.total_tokens if response.usage else 0,
            },
            finish_reason=choice.finish_reason or "stop",
        )
    
    async def generate_stream(self, request: GenerateRequest) -> AsyncIterator[str]:
        """Stream using OpenAI API."""
        try:
            from openai import AsyncOpenAI
        except ImportError:
            raise ImportError("pip install openai")
        
        client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)
        
        stream = await client.chat.completions.create(
            model=request.model or "gpt-4o-mini",
            messages=request.messages,
            temperature=request.temperature,
            stream=True,
        )
        
        async for chunk in stream:
            delta = chunk.choices[0].delta
            if delta.content:
                yield delta.content
    
    async def embed(self, texts: list[str], model: str = "") -> list[list[float]]:
        try:
            from openai import AsyncOpenAI
        except ImportError:
            raise ImportError("pip install openai")
        
        client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)
        response = await client.embeddings.create(
            model=model or "text-embedding-3-small",
            input=texts,
        )
        return [item.embedding for item in response.data]


# ─── Ollama Provider ────────────────────────────────────────────────────────

class OllamaProvider(ModelProvider):
    """Local Ollama provider."""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
    
    @property
    def name(self) -> str:
        return "ollama"
    
    def list_models(self) -> list[ModelConfig]:
        # Dynamic — would query Ollama API in production
        return [
            ModelConfig("llama3.1", "ollama", ModelType.LLM, "Llama 3.1",
                       context_window=131072),
            ModelConfig("nomic-embed-text", "ollama", ModelType.EMBEDDING,
                       "Nomic Embed Text"),
        ]
    
    async def generate(self, request: GenerateRequest) -> GenerateResponse:
        import aiohttp
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": request.model or "llama3.1",
                    "messages": request.messages,
                    "stream": False,
                    "options": {"temperature": request.temperature},
                },
            ) as resp:
                data = await resp.json()
                return GenerateResponse(
                    content=data.get("message", {}).get("content", ""),
                    model=data.get("model", request.model),
                )
    
    async def generate_stream(self, request: GenerateRequest) -> AsyncIterator[str]:
        import aiohttp
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": request.model or "llama3.1",
                    "messages": request.messages,
                    "stream": True,
                },
            ) as resp:
                import json
                async for line in resp.content:
                    text = line.decode().strip()
                    if text:
                        data = json.loads(text)
                        content = data.get("message", {}).get("content", "")
                        if content:
                            yield content


# ─── Model Registry ─────────────────────────────────────────────────────────
# @source dify/api/core/model_runtime/model_provider/

class ModelRegistry:
    """
    Registry of model providers.
    
    @example
    registry = ModelRegistry()
    registry.register(OpenAIProvider(api_key="sk-..."))
    registry.register(OllamaProvider())
    
    response = await registry.generate(GenerateRequest(
        model="gpt-4o-mini", messages=[...]
    ))
    """
    
    def __init__(self):
        self._providers: dict[str, ModelProvider] = {}
        self._model_to_provider: dict[str, str] = {}
    
    def register(self, provider: ModelProvider) -> None:
        """Register a model provider."""
        self._providers[provider.name] = provider
        for model in provider.list_models():
            self._model_to_provider[model.model_id] = provider.name
        logger.info(f"Registered provider: {provider.name}")
    
    def get_provider(self, model_id: str) -> ModelProvider:
        """Get the provider for a model ID."""
        provider_name = self._model_to_provider.get(model_id)
        if not provider_name:
            raise ValueError(f"No provider registered for model: {model_id}")
        return self._providers[provider_name]
    
    def list_all_models(self) -> list[ModelConfig]:
        """List all available models across all providers."""
        models = []
        for provider in self._providers.values():
            models.extend(provider.list_models())
        return models
    
    async def generate(self, request: GenerateRequest) -> GenerateResponse:
        """Route a generate request to the appropriate provider."""
        provider = self.get_provider(request.model)
        return await provider.generate(request)
    
    async def generate_stream(
        self, request: GenerateRequest
    ) -> AsyncIterator[str]:
        """Route a streaming request to the appropriate provider."""
        provider = self.get_provider(request.model)
        async for token in provider.generate_stream(request):
            yield token
