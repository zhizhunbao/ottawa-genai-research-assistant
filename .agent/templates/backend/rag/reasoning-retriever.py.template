"""
Reasoning Retriever - LLM-powered tree search without vectors or chunking.

@module backend/rag/reasoning-retriever
@source pageindex/pageindex/page_index.py (1144L)
@reference https://github.com/EricLiu750501/PageIndex
@template S3-M4-F1

Inspired by AlphaGo's tree search algorithm, this retriever navigates a
document's hierarchical TOC structure using LLM reasoning to find the most
relevant sections. No embeddings, no chunking — only the document's outline
and LLM intelligence.

Key concepts:
1. Document → hierarchical TOC tree (auto-generated or extracted)
2. Query → LLM reasons about which branch to explore
3. Traverse tree → expand most promising nodes
4. Retrieve full page text once target section is located

Achieves 98.7% accuracy on FinanceBench.
"""

from __future__ import annotations

import asyncio
import json
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any, AsyncIterable, Optional

logger = logging.getLogger(__name__)


# ─── Types ───────────────────────────────────────────────────────────────────

@dataclass
class TOCNode:
    """A node in the document's hierarchical table of contents."""
    structure: str  # e.g., "1.2.3"
    title: str
    physical_index: int | None = None  # 1-based page number
    children: list["TOCNode"] = field(default_factory=list)
    content: str | None = None  # Page text (populated on demand)
    score: float = 0.0  # Relevance score from LLM reasoning


@dataclass
class RetrievalResult:
    """Result from the reasoning retriever."""
    node: TOCNode
    page_text: str
    reasoning: str  # LLM's reasoning about why this section is relevant
    confidence: float  # 0-1 confidence score


# ─── LLM Interface ──────────────────────────────────────────────────────────

class LLMClient(ABC):
    """Abstract LLM client for reasoning calls."""
    
    @abstractmethod
    async def complete(self, prompt: str, **kwargs) -> str:
        """Send a prompt to the LLM and return the response text."""
        ...


# ─── Reasoning Retriever ────────────────────────────────────────────────────
# @source pageindex/page_index.py — tree search pattern (L453-587)

class ReasoningRetriever:
    """
    LLM-powered tree search retriever that navigates document structure
    without vectors or chunking.
    
    Algorithm:
    1. Present the TOC root nodes to the LLM with the query
    2. LLM selects the most relevant branch(es) with reasoning
    3. Expand selected nodes, present children
    4. Repeat until leaf nodes (pages) are reached
    5. Retrieve full text of selected pages
    
    @source pageindex/page_index.py check_title_appearance (L13-45)
    @source pageindex/page_index.py process_no_toc (L568-587)
    """
    
    def __init__(
        self,
        llm: LLMClient,
        max_depth: int = 5,
        top_k: int = 3,
        concurrent_requests: int = 5,
    ):
        self.llm = llm
        self.max_depth = max_depth
        self.top_k = top_k
        self.concurrent_requests = concurrent_requests
    
    async def retrieve(
        self,
        query: str,
        toc: list[TOCNode],
        page_texts: dict[int, str],
    ) -> list[RetrievalResult]:
        """
        Navigate the TOC tree to find sections relevant to the query.
        
        Args:
            query: User's question
            toc: Hierarchical table of contents
            page_texts: Dict mapping page_index → full page text
            
        Returns:
            List of RetrievalResult sorted by confidence
        """
        selected_nodes = await self._tree_search(query, toc, depth=0)
        
        results = []
        for node, reasoning, confidence in selected_nodes:
            page_text = page_texts.get(node.physical_index, "")
            results.append(RetrievalResult(
                node=node,
                page_text=page_text,
                reasoning=reasoning,
                confidence=confidence,
            ))
        
        # Sort by confidence descending
        results.sort(key=lambda r: r.confidence, reverse=True)
        return results[:self.top_k]
    
    async def _tree_search(
        self,
        query: str,
        nodes: list[TOCNode],
        depth: int,
    ) -> list[tuple[TOCNode, str, float]]:
        """
        Recursively search the TOC tree using LLM reasoning.
        
        @source pageindex/page_index.py generate_toc_init/continue pattern
        """
        if depth >= self.max_depth or not nodes:
            return []
        
        # Step 1: Ask LLM which nodes are relevant
        selected = await self._select_relevant_nodes(query, nodes)
        
        results = []
        for node, reasoning, confidence in selected:
            if node.children:
                # Has children → recurse deeper
                child_results = await self._tree_search(
                    query, node.children, depth + 1
                )
                if child_results:
                    results.extend(child_results)
                else:
                    # Children didn't yield results, use this node
                    results.append((node, reasoning, confidence))
            else:
                # Leaf node → this is our target
                results.append((node, reasoning, confidence))
        
        return results
    
    async def _select_relevant_nodes(
        self,
        query: str,
        nodes: list[TOCNode],
    ) -> list[tuple[TOCNode, str, float]]:
        """
        Ask the LLM to select relevant nodes from a list of candidates.
        
        @source pageindex/page_index.py check_title_appearance (L13-45)
        """
        node_list = "\n".join(
            f"  [{i}] {n.structure} — {n.title} (page {n.physical_index})"
            for i, n in enumerate(nodes)
        )
        
        prompt = f"""You are a document search expert. Given a user's question and a list of 
document sections, select the most relevant sections that are likely to contain 
the answer.

User question: {query}

Available sections:
{node_list}

For each relevant section, explain your reasoning and provide a confidence score (0-1).

Reply in JSON format:
[
    {{
        "index": <section index number>,
        "reasoning": "<why this section is relevant>",
        "confidence": <0.0 to 1.0>
    }}
]

Select up to {self.top_k} most relevant sections. Only include genuinely relevant sections.
Return the JSON array directly."""

        response = await self.llm.complete(prompt)
        
        try:
            selections = json.loads(self._extract_json(response))
        except (json.JSONDecodeError, ValueError):
            logger.warning("Failed to parse LLM response for node selection")
            return []
        
        results = []
        for sel in selections:
            idx = sel.get("index", -1)
            if 0 <= idx < len(nodes):
                results.append((
                    nodes[idx],
                    sel.get("reasoning", ""),
                    float(sel.get("confidence", 0.5)),
                ))
        
        return results
    
    @staticmethod
    def _extract_json(text: str) -> str:
        """Extract JSON content from LLM response."""
        # Try to find JSON array
        text = text.strip()
        if text.startswith("```json"):
            text = text[7:]
        if text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        return text.strip()


# ─── TOC Builder ─────────────────────────────────────────────────────────────
# @source pageindex/page_index.py generate_toc_init (L534-566) + 
#         generate_toc_continue (L499-531) + process_no_toc (L568-587)

class TOCBuilder:
    """
    Automatically builds a hierarchical TOC from document pages using LLM.
    
    Pattern: progressively build TOC by feeding page groups to LLM,
    then continue building from where the previous batch stopped.
    """
    
    def __init__(
        self,
        llm: LLMClient,
        max_tokens_per_group: int = 20000,
    ):
        self.llm = llm
        self.max_tokens_per_group = max_tokens_per_group
    
    async def build_toc(self, page_texts: dict[int, str]) -> list[TOCNode]:
        """
        Build TOC from page texts using progressive LLM extraction.
        
        @source pageindex/page_index.py process_no_toc (L568-587)
        """
        sorted_pages = sorted(page_texts.keys())
        
        # Group pages into chunks that fit within token limits
        groups = self._group_pages(sorted_pages, page_texts)
        
        if not groups:
            return []
        
        # Initialize TOC from first group
        toc_data = await self._extract_toc_init(groups[0])
        
        # Continue building from subsequent groups
        for group in groups[1:]:
            additional = await self._extract_toc_continue(toc_data, group)
            toc_data.extend(additional)
        
        # Convert flat list to tree
        return self._build_tree(toc_data)
    
    def _group_pages(
        self,
        page_indices: list[int],
        page_texts: dict[int, str],
    ) -> list[str]:
        """Group pages into text chunks within token limits."""
        groups = []
        current_group = ""
        current_len = 0
        
        for idx in page_indices:
            text = page_texts[idx]
            page_content = f"<page_{idx}>\n{text}\n</page_{idx}>\n\n"
            estimated_tokens = len(page_content) // 4  # rough estimate
            
            if current_len + estimated_tokens > self.max_tokens_per_group:
                if current_group:
                    groups.append(current_group)
                current_group = page_content
                current_len = estimated_tokens
            else:
                current_group += page_content
                current_len += estimated_tokens
        
        if current_group:
            groups.append(current_group)
        
        return groups
    
    async def _extract_toc_init(self, text: str) -> list[dict]:
        """
        Extract initial TOC from first page group.
        @source pageindex/page_index.py generate_toc_init (L534-566)
        """
        prompt = f"""Extract the hierarchical document structure from the following text.

For each section, provide:
- structure: hierarchical index (e.g., "1", "1.1", "1.1.1")
- title: section title (preserve original)
- physical_index: page number where section starts

Reply as a JSON array:
[
    {{"structure": "1", "title": "Introduction", "physical_index": 1}},
    {{"structure": "1.1", "title": "Background", "physical_index": 2}},
    ...
]

Text:
{text}

Return only the JSON array."""

        response = await self.llm.complete(prompt)
        try:
            return json.loads(ReasoningRetriever._extract_json(response))
        except json.JSONDecodeError:
            return []
    
    async def _extract_toc_continue(
        self, existing_toc: list[dict], text: str
    ) -> list[dict]:
        """
        Continue TOC extraction for subsequent page groups.
        @source pageindex/page_index.py generate_toc_continue (L499-531)
        """
        prompt = f"""Continue extracting the document structure. The previous sections are shown below.
Your task is to extract ONLY the new sections found in the current text.

Previous structure:
{json.dumps(existing_toc, indent=2)}

Current text:
{text}

Extract only the new sections. Return a JSON array of new sections only."""

        response = await self.llm.complete(prompt)
        try:
            return json.loads(ReasoningRetriever._extract_json(response))
        except json.JSONDecodeError:
            return []
    
    @staticmethod
    def _build_tree(flat_items: list[dict]) -> list[TOCNode]:
        """Convert flat TOC list into hierarchical tree."""
        nodes = []
        node_map: dict[str, TOCNode] = {}
        
        for item in flat_items:
            structure = item.get("structure", "")
            node = TOCNode(
                structure=structure,
                title=item.get("title", ""),
                physical_index=item.get("physical_index"),
            )
            node_map[structure] = node
            
            # Find parent
            parts = structure.rsplit(".", 1)
            if len(parts) == 2:
                parent_key = parts[0]
                parent = node_map.get(parent_key)
                if parent:
                    parent.children.append(node)
                    continue
            
            nodes.append(node)
        
        return nodes
