"""
RAG Strategy Abstract Base Class — Strategy Pattern for RAG approaches.

Defines the contract for all RAG (Retrieval-Augmented Generation) approaches:
- `run()` for non-streaming responses
- `run_stream()` for streaming responses
- Common search, embedding, and query rewriting utilities

This pattern allows swapping between different RAG strategies
(simple retrieval, multi-turn chat, hybrid search, agentic retrieval)
without changing the calling code.

Source: azure-search-openai-demo/app/backend/approaches/approach.py (1020 lines)
Reference: https://github.com/Azure-Samples/azure-search-openai-demo
Template: S3-M1-F1

Usage:
    class MyChatApproach(Approach):
        async def run(self, messages, session_state=None, context={}):
            # 1. Rewrite query
            # 2. Search documents
            # 3. Generate answer with context
            return {"message": {"content": answer, "role": "assistant"}, ...}

        async def run_stream(self, messages, session_state=None, context={}):
            yield {"delta": {"role": "assistant"}}
            yield {"delta": {"content": chunk}}
"""

import json
import logging
import re
from abc import ABC
from collections.abc import AsyncGenerator, Awaitable
from dataclasses import asdict, dataclass, field
from typing import Any, Optional

from openai import AsyncOpenAI, AsyncStream
from openai.types import CompletionUsage
from openai.types.chat import (
    ChatCompletion,
    ChatCompletionChunk,
    ChatCompletionMessageParam,
    ChatCompletionToolParam,
)

logger = logging.getLogger(__name__)


# ==================== Data Classes ====================


@dataclass
class Document:
    """A retrieved document with metadata and scoring.

    Source: approach.py L62-104
    """

    id: Optional[str] = None
    content: Optional[str] = None
    category: Optional[str] = None
    sourcepage: Optional[str] = None
    sourcefile: Optional[str] = None
    score: Optional[float] = None
    reranker_score: Optional[float] = None

    def serialize_for_results(self) -> dict[str, Any]:
        return {
            "id": self.id,
            "content": self.content,
            "category": self.category,
            "sourcepage": self.sourcepage,
            "sourcefile": self.sourcefile,
            "score": self.score,
            "reranker_score": self.reranker_score,
        }


@dataclass
class ThoughtStep:
    """A step in the RAG reasoning chain, used for explainability.

    Source: approach.py L155-163
    """

    title: str
    description: Optional[Any]
    props: Optional[dict[str, Any]] = None

    def update_token_usage(self, usage: CompletionUsage) -> None:
        if self.props is None:
            self.props = {}
        self.props["token_usage"] = {
            "prompt_tokens": usage.prompt_tokens,
            "completion_tokens": usage.completion_tokens,
            "total_tokens": usage.total_tokens,
        }


@dataclass
class DataPoints:
    """Source data points supporting an answer.

    Source: approach.py L180-186
    """

    text: Optional[list[str]] = None
    images: Optional[list[str]] = None
    citations: Optional[list[str]] = None


@dataclass
class ExtraInfo:
    """Extra information returned alongside the answer.

    Source: approach.py L189-194
    """

    data_points: DataPoints
    thoughts: list[ThoughtStep] = field(default_factory=list)
    followup_questions: Optional[list[Any]] = None


# ==================== Abstract Base Class ====================


class Approach(ABC):
    """Base class for all RAG approach strategies.

    Provides common utilities for:
    - Search (text, vector, hybrid, semantic)
    - Embedding computation
    - Query rewriting via LLM
    - Citation management
    - Chat completion creation (streaming + non-streaming)

    Source: approach.py L224-1020
    Simplified: Removed Azure-specific agentic retrieval, blob storage,
    SharePoint/web sources, image embeddings. Kept core RAG patterns.

    Subclasses must implement:
    - run() — non-streaming execution
    - run_stream() — streaming execution
    """

    # Token limit defaults
    # Source: approach.py L236-237
    RESPONSE_DEFAULT_TOKEN_LIMIT = 1024
    RESPONSE_REASONING_TOKEN_LIMIT = 8192

    def __init__(
        self,
        *,
        openai_client: AsyncOpenAI,
        chatgpt_model: str,
        chatgpt_deployment: Optional[str] = None,
        embedding_model: str = "text-embedding-3-small",
        embedding_dimensions: int = 1536,
    ):
        """Initialize the approach with OpenAI client and model configuration.

        Args:
            openai_client: Async OpenAI client instance.
            chatgpt_model: Model name for chat completions.
            chatgpt_deployment: Azure deployment name (None for non-Azure).
            embedding_model: Model name for embeddings.
            embedding_dimensions: Embedding vector dimensions.

        Source: approach.py L240-282 (simplified)
        """
        self.openai_client = openai_client
        self.chatgpt_model = chatgpt_model
        self.chatgpt_deployment = chatgpt_deployment
        self.embedding_model = embedding_model
        self.embedding_dimensions = embedding_dimensions
        self.include_token_usage = True

    # ==================== Search ====================

    def build_filter(self, overrides: dict[str, Any]) -> Optional[str]:
        """Build a search filter from override parameters.

        Source: approach.py L284-292
        """
        include_category = overrides.get("include_category")
        exclude_category = overrides.get("exclude_category")
        filters = []
        if include_category:
            filters.append(
                "category eq '{}'".format(include_category.replace("'", "''"))
            )
        if exclude_category:
            filters.append(
                "category ne '{}'".format(exclude_category.replace("'", "''"))
            )
        return None if not filters else " and ".join(filters)

    # ==================== Embedding ====================

    async def compute_text_embedding(self, query: str) -> list[float]:
        """Compute text embedding for a search query.

        Source: approach.py L879-901
        """
        embedding = await self.openai_client.embeddings.create(
            model=self.chatgpt_deployment or self.embedding_model,
            input=query,
            dimensions=self.embedding_dimensions,
        )
        return embedding.data[0].embedding

    # ==================== Query Rewriting ====================

    def extract_rewritten_query(
        self,
        chat_completion: ChatCompletion,
        user_query: str,
    ) -> str:
        """Extract rewritten search query from LLM response.

        The LLM may return the optimized query as:
        1. A tool call argument (preferred)
        2. Direct text content (fallback)

        Source: approach.py L365-391
        """
        response_message = chat_completion.choices[0].message

        # Check tool calls first
        if response_message.tool_calls:
            for tool_call in response_message.tool_calls:
                if tool_call.type != "function":
                    continue
                try:
                    parsed = json.loads(tool_call.function.arguments or "{}")
                    search_query = parsed.get("search_query")
                    if search_query:
                        return search_query
                except json.JSONDecodeError:
                    continue

        # Fallback to content
        if response_message.content:
            candidate = response_message.content.strip()
            if candidate:
                return candidate

        return user_query

    # ==================== Sources & Citations ====================

    def get_sources_content(
        self,
        results: list[Document],
        use_semantic_captions: bool = False,
    ) -> DataPoints:
        """Extract text sources and citations from search results.

        Source: approach.py L720-828 (simplified)
        """
        citations: list[str] = []
        text_sources: list[str] = []

        for doc in results:
            citation = doc.sourcepage or ""
            if citation and citation not in citations:
                citations.append(citation)

            content = doc.content or ""
            # Clean source text
            content = content.replace("\n", " ").replace("\r", " ")
            text_sources.append(f"{citation}: {content}")

        return DataPoints(text=text_sources, citations=citations)

    # ==================== Chat Completion ====================

    def create_chat_completion(
        self,
        messages: list[ChatCompletionMessageParam],
        overrides: dict[str, Any],
        response_token_limit: int = 1024,
        should_stream: bool = False,
        tools: Optional[list[ChatCompletionToolParam]] = None,
        temperature: Optional[float] = None,
    ) -> Awaitable[ChatCompletion] | Awaitable[AsyncStream[ChatCompletionChunk]]:
        """Create an OpenAI chat completion (streaming or non-streaming).

        Source: approach.py L934-981
        """
        params: dict[str, Any] = {
            "max_tokens": response_token_limit,
            "temperature": temperature or overrides.get("temperature", 0.3),
        }

        if should_stream:
            params["stream"] = True
            params["stream_options"] = {"include_usage": True}

        if tools is not None:
            params["tools"] = tools

        model = self.chatgpt_deployment or self.chatgpt_model

        return self.openai_client.chat.completions.create(
            model=model,
            messages=messages,
            n=1,
            **params,
        )

    def format_thought_step(
        self,
        title: str,
        messages: list[ChatCompletionMessageParam],
        model: str,
        usage: Optional[CompletionUsage] = None,
    ) -> ThoughtStep:
        """Format a thought step for explainability.

        Source: approach.py L983-1003
        """
        props: dict[str, Any] = {"model": model}
        step = ThoughtStep(title, messages, props)
        if usage:
            step.update_token_usage(usage)
        return step

    # ==================== Abstract Interface ====================

    async def run(
        self,
        messages: list[ChatCompletionMessageParam],
        session_state: Any = None,
        context: dict[str, Any] = {},
    ) -> dict[str, Any]:
        """Execute the RAG approach (non-streaming).

        Source: approach.py L1005-1011

        Returns:
            dict with "message", "context", and "session_state" keys.
        """
        raise NotImplementedError

    async def run_stream(
        self,
        messages: list[ChatCompletionMessageParam],
        session_state: Any = None,
        context: dict[str, Any] = {},
    ) -> AsyncGenerator[dict[str, Any], None]:
        """Execute the RAG approach (streaming).

        Source: approach.py L1013-1019

        Yields:
            dicts with "delta" and optional "context"/"session_state" keys.
        """
        raise NotImplementedError
